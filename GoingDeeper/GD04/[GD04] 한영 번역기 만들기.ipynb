{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3690fc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# í•œêµ­ì–´ë¥¼ ì§€ì›í•˜ëŠ” í°íŠ¸ë¡œ ë³€ê²½\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager.findfont(font)\n",
    "\n",
    "print(\"ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6b11cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da7569b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì••ì¶• í•´ì œ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "# ì••ì¶• í’€ê¸°\n",
    "tar_path = \"./data/korean-english-park.train.tar.gz\"\n",
    "extract_path = \"./data/korean-english-park/\"\n",
    "\n",
    "with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "    tar.extractall(path=extract_path)\n",
    "\n",
    "print(\"ì••ì¶• í•´ì œ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e67f761c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['korean-english-park.train.en', 'korean-english-park.train.ko']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.listdir(extract_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27dad56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "íŒŒì¼ ë³€í™˜ ì™„ë£Œ! 'korean.txt'ì™€ 'english.txt'ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "kor_file = \"./data/korean-english-park/korean-english-park.train.ko\"\n",
    "eng_file = \"./data/korean-english-park/korean-english-park.train.en\"\n",
    "\n",
    "# ë³€í™˜ëœ íŒŒì¼ ì €ì¥ ê²½ë¡œ\n",
    "kor_output = \"./data/korean.txt\"\n",
    "eng_output = \"./data/english.txt\"\n",
    "\n",
    "# í•œêµ­ì–´, ì˜ì–´ ë¬¸ì¥ì„ ê°ê° ì €ì¥\n",
    "with open(kor_file, \"r\", encoding=\"utf-8\") as f_ko, open(eng_file, \"r\", encoding=\"utf-8\") as f_en:\n",
    "    korean_sentences = f_ko.readlines()\n",
    "    english_sentences = f_en.readlines()\n",
    "\n",
    "# ìƒˆë¡œìš´ íŒŒì¼ë¡œ ì €ì¥\n",
    "with open(kor_output, \"w\", encoding=\"utf-8\") as f_ko, open(eng_output, \"w\", encoding=\"utf-8\") as f_en:\n",
    "    f_ko.writelines(korean_sentences)\n",
    "    f_en.writelines(english_sentences)\n",
    "\n",
    "print(\"íŒŒì¼ ë³€í™˜ ì™„ë£Œ! 'korean.txt'ì™€ 'english.txt'ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c18b76c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a70203d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import MeCab\n",
    "\n",
    "def clean_text(sentence, lang=\"ko\"):\n",
    "    \"\"\"íŠ¹ìˆ˜ ë¬¸ì ë° ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°\"\"\"\n",
    "    if lang == \"ko\":\n",
    "        sentence = re.sub(r\"[^ê°€-í£ã„±-ã…ã…-ã…£\\s]\", \"\", sentence)  # í•œê¸€ê³¼ ê³µë°±ë§Œ ë‚¨ê¸°ê¸°\n",
    "    else:  # ì˜ì–´\n",
    "        sentence = sentence.lower().strip()  # ì†Œë¬¸ìë¡œ ë³€í™˜ ë° ê³µë°± ì œê±°\n",
    "        sentence = re.sub(r\"[^a-zA-Z?.!,Â¿]\", \" \", sentence)  # íŠ¹ìˆ˜ ë¬¸ì ì œê±°\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence).strip()  # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784096d5",
   "metadata": {},
   "source": [
    "# Step2 : ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3180f6ca",
   "metadata": {},
   "source": [
    "### ì •ì œí•˜ê¸° & í† í°í™”í•˜ê¸°\n",
    "- ë°ì´í„°ëŠ” \\t ê¸°í˜¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì˜ì–´ì™€ ìŠ¤í˜ì¸ì–´ê°€ ë³‘ë ¬ ìŒ\n",
    "- \\t ê¸°í˜¸ë¥¼ ë§¤ê°œë³€ìˆ˜ë¡œ split() í•¨ìˆ˜ë¥¼ í˜¸ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e62b9cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import MeCab\n",
    "\n",
    "# MeCab í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”\n",
    "mecab = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "def clean_text(sentence, lang=\"ko\"):\n",
    "    \"\"\"íŠ¹ìˆ˜ ë¬¸ì ë° ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°\"\"\"\n",
    "    if lang == \"ko\":\n",
    "        sentence = re.sub(r\"[^ê°€-í£ã„±-ã…ã…-ã…£\\s]\", \"\", sentence)  # í•œê¸€ê³¼ ê³µë°±ë§Œ ë‚¨ê¸°ê¸°\n",
    "    else:  # ì˜ì–´\n",
    "        sentence = sentence.lower().strip()  # ì†Œë¬¸ìë¡œ ë³€í™˜ ë° ê³µë°± ì œê±°\n",
    "        sentence = re.sub(r\"[^a-zA-Z?.!,Â¿]\", \" \", sentence)  # íŠ¹ìˆ˜ ë¬¸ì ì œê±°\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence).strip()  # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°\n",
    "    return sentence\n",
    "\n",
    "def tokenize_text(sentence, lang=\"ko\"):\n",
    "    \"\"\"í† í°í™” ìˆ˜í–‰\"\"\"\n",
    "    if lang == \"ko\":\n",
    "        return mecab.parse(sentence).strip()  # MeCab í† í°í™”\n",
    "    else:  # ì˜ì–´\n",
    "        return \"<start> \" + sentence + \" <end>\"  # ì‹œì‘ê³¼ ë í† í° ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e440602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë°ì´í„° ì •ì œ ë° í† í°í™” ì™„ë£Œ!\n",
      "ğŸ”¹ ì¤‘ë³µ ì œê±° í›„ ë°ì´í„° ê°œìˆ˜: 66658\n",
      "ğŸ”¹ í•œêµ­ì–´ ìƒ˜í”Œ: ['ì¿ ë°” ì„œë¶€ ì§€ì—­ ì˜ í† ì–‘ ì€ ì„¸ê³„ ì—ì„œ ë‹´ë°° ë¥¼ ê¸°ë¥´ ëŠ” ìµœì  ì˜ ì¥ì†Œ ì´ ë‹¤', 'ë‚¨ë¶í•œ ì€ ë…„ ì—­ì‚¬ ì  ì¸ ì •ìƒ íšŒë‹´ ìœ¼ë¡œ ë‚¨ë¶ ê´€ê³„ ê°€ í•´ë¹™ ë˜ ê¸° ì „ ìˆ˜ ì‹­ ë…„ ë™ì•ˆ ì„œë¡œ ë¥¼ ë¹„ë°© í–ˆ ë‹¤', 'ë§¤ì¼€ì¸ ì€ ìš°ë¦¬ ëŠ” ìƒˆë¡­ ê³  ê°„ë‹¨ í•œ ì„¸ê¸ˆ ì‹œìŠ¤í…œ ì„ ë§Œë“¤ ì–´ ë¯¸êµ­ì¸ ë“¤ ì—ê²Œ ê¸°íšŒ ë¥¼ ì£¼ ë ¤ê³  í•œë‹¤ê³  ë§ë¶™ì˜€ ë‹¤']\n",
      "ğŸ”¹ ì˜ì–´ ìƒ˜í”Œ: ['<start> the soil in the western part of the country is considered the best in the world for growing tobacco. <end>', '<start> before their historic summit in thawed relations, the two koreas had vilified each other for decades. <end>', '<start> we are going to create a new and simpler tax system and give the american people a choice. <end>']\n"
     ]
    }
   ],
   "source": [
    "# íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "kor_path = \"./data/korean.txt\"\n",
    "eng_path = \"./data/english.txt\"\n",
    "\n",
    "# ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ set í™œìš©\n",
    "unique_pairs = set()\n",
    "\n",
    "with open(kor_path, \"r\", encoding=\"utf-8\") as f_kor, open(eng_path, \"r\", encoding=\"utf-8\") as f_eng:\n",
    "    for kor, eng in zip(f_kor, f_eng):\n",
    "        kor_clean = clean_text(kor.strip(), lang=\"ko\")\n",
    "        eng_clean = clean_text(eng.strip(), lang=\"en\")\n",
    "\n",
    "        kor_tokenized = tokenize_text(kor_clean, lang=\"ko\")\n",
    "        eng_tokenized = tokenize_text(eng_clean, lang=\"en\")\n",
    "\n",
    "        # ë³‘ë ¬ ë°ì´í„° ìŒì„ (í•œêµ­ì–´, ì˜ì–´) í˜•íƒœë¡œ ì €ì¥\n",
    "        unique_pairs.add((kor_tokenized, eng_tokenized))\n",
    "\n",
    "# ì •ì œëœ ë°ì´í„° ì €ì¥ (ê¸¸ì´ í•„í„°ë§ í¬í•¨)\n",
    "cleaned_corpus = []\n",
    "eng_corpus = []\n",
    "kor_corpus = []\n",
    "\n",
    "for kor, eng in unique_pairs:\n",
    "    if len(kor.split()) <= 40 and len(eng.split()) <= 40:  # ê¸¸ì´ ì œí•œ ì ìš©\n",
    "        cleaned_corpus.append((kor, eng))\n",
    "        kor_corpus.append(kor)\n",
    "        eng_corpus.append(eng)\n",
    "\n",
    "print(\"âœ… ë°ì´í„° ì •ì œ ë° í† í°í™” ì™„ë£Œ!\")\n",
    "print(f\"ğŸ”¹ ì¤‘ë³µ ì œê±° í›„ ë°ì´í„° ê°œìˆ˜: {len(cleaned_corpus)}\")\n",
    "print(\"ğŸ”¹ í•œêµ­ì–´ ìƒ˜í”Œ:\", kor_corpus[:3])\n",
    "print(\"ğŸ”¹ ì˜ì–´ ìƒ˜í”Œ:\", eng_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55689657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•œêµ­ì–´ vocab size: 38544\n",
      "ì˜ì–´ vocab size: 61539\n",
      "Training samples: 66658\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# í† í°í™” í•¨ìˆ˜\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')   # íŠ¹ìˆ˜ë¬¸ì í•„í„°ë§ X, num_words ì¶”í›„ ì„¤ì •\n",
    "    tokenizer.fit_on_texts(corpus)  # ë‹¨ì–´ ì‚¬ì „ êµ¬ì¶•\n",
    " \n",
    "    tensor = tokenizer.texts_to_sequences(corpus)  # ë¬¸ì¥ì„ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  # íŒ¨ë”© ì¶”ê°€\n",
    "\n",
    "    return tensor, tokenizer\n",
    "\n",
    "# í•œêµ­ì–´ ë¬¸ì¥ (ì…ë ¥) í† í°í™”\n",
    "enc_tensor, enc_tokenizer = tokenize(kor_corpus)\n",
    "\n",
    "# ì˜ì–´ ë¬¸ì¥ (ì¶œë ¥) í† í°í™”\n",
    "dec_tensor, dec_tokenizer = tokenize(eng_corpus)\n",
    "\n",
    "# ë°ì´í„° í™•ì¸\n",
    "print(\"í•œêµ­ì–´ vocab size:\", len(enc_tokenizer.word_index) + 1)  # íŒ¨ë”© í¬í•¨\n",
    "print(\"ì˜ì–´ vocab size:\", len(dec_tokenizer.word_index) + 1)\n",
    "print(\"Training samples:\", len(enc_tensor))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc25e05",
   "metadata": {},
   "source": [
    "# Step3 : ëª¨ë¸ ì„¤ê³„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4ee0706",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vec = attn * h_enc\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "\n",
    "        return context_vec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3832bc6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aec904d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units,\n",
    "                                       return_sequences=True)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.gru(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ba57d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        context_vec, attn = self.attention(enc_out, h_dec)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
    "        \n",
    "        out, h_dec = self.gru(out)\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, h_dec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a85f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b369869b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output: (32, 30, 1024)\n",
      "Decoder Output: (32, 61539)\n",
      "Decoder Hidden State: (32, 1024)\n",
      "Attention: (32, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "# ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\n",
    "\n",
    "BATCH_SIZE     = 32\n",
    "SRC_VOCAB_SIZE = len(enc_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(dec_tokenizer.index_word) + 1\n",
    "\n",
    "units         = 1024\n",
    "embedding_dim = 512\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "\n",
    "# sample input\n",
    "sequence_len = 30\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_output = encoder(sample_enc)\n",
    "\n",
    "print ('Encoder Output:', sample_output.shape)\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                     sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('Decoder Hidden State:', h_dec.shape)\n",
    "print ('Attention:', attn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2cb18e",
   "metadata": {},
   "source": [
    "### Optimizer & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1ee8914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizerì™€ Loss ì •ì˜\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))  # 0ì„ íŒ¨ë”©ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ë§ˆìŠ¤í¬ ìƒì„±\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask  # ë§ˆìŠ¤í¬ë¥¼ ê³±í•˜ì—¬ íŒ¨ë”© ë¶€ë¶„ì„ ì œì™¸í•œ ì†ì‹¤ ê³„ì‚°\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b070a830",
   "metadata": {},
   "source": [
    "### train step êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd446f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tokenizer):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_out = encoder(src)  # Encoderì˜ ì¶œë ¥\n",
    "        h_dec = enc_out[:, -1]  # ë§ˆì§€ë§‰ hidden state\n",
    "        \n",
    "        dec_src = tf.expand_dims([dec_tokenizer.word_index['<start>']] * bsz, 1)  # <start> í† í°ì„ decoderì— ì „ë‹¬\n",
    "\n",
    "        for t in range(1, tgt.shape[1]):  # t: ì‹œí€€ìŠ¤ì˜ ê° timestep\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)  # Decoder ì‹¤í–‰\n",
    "\n",
    "            loss += loss_function(tgt[:, t], pred)  # ì†ì‹¤ ê³„ì‚°\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)  # ë‹¤ìŒ time stepìœ¼ë¡œ ì´ë™\n",
    "\n",
    "    batch_loss = loss / int(tgt.shape[1])  # ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë”°ë¥¸ í‰ê·  ì†ì‹¤ ê³„ì‚°\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables  # í•™ìŠµ ê°€ëŠ¥í•œ ë³€ìˆ˜ë“¤\n",
    "    gradients = tape.gradient(loss, variables)  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°\n",
    "    optimizer.apply_gradients(zip(gradients, variables))  # ê·¸ë˜ë””ì–¸íŠ¸ ì ìš©\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a5a1a0",
   "metadata": {},
   "source": [
    "### í›ˆë ¨ ì‹œì‘í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10b7f399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2084 [00:51<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "   OOM when allocating tensor with shape[1024,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node split_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[decoder_1/gru_3/PartitionedCall_24]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_step_144992]\n\nFunction call stack:\ntrain_step -> train_step -> train_step\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_291/2751730745.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         batch_loss = train_step(enc_tensor[idx:idx+BATCH_SIZE],\n\u001b[0m\u001b[1;32m     16\u001b[0m                                 \u001b[0mdec_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                 \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:    OOM when allocating tensor with shape[1024,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node split_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[decoder_1/gru_3/PartitionedCall_24]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_step_144992]\n\nFunction call stack:\ntrain_step -> train_step -> train_step\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm \n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "# í›ˆë ¨ ë£¨í”„\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_tensor.shape[0], BATCH_SIZE))  # ë°ì´í„° ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸\n",
    "    random.shuffle(idx_list)  # ë°ì´í„° ì…”í”Œ\n",
    "    t = tqdm(idx_list)  # ì§„í–‰ í‘œì‹œì¤„\n",
    "\n",
    "    for batch, idx in enumerate(t):\n",
    "        batch_loss = train_step(enc_tensor[idx:idx+BATCH_SIZE],\n",
    "                                dec_tensor[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)  # í›ˆë ¨ ë‹¨ê³„ ì‹¤í–‰\n",
    "    \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        # tqdm ì—…ë°ì´íŠ¸\n",
    "        t.set_description(f'Epoch {epoch + 1}')  # ì—í­ í‘œì‹œ\n",
    "        t.set_postfix(loss=f'{total_loss.numpy() / (batch + 1):.4f}')  # í˜„ì¬ ì—í­ì— ëŒ€í•œ ì†ì‹¤ í‘œì‹œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d22317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í›ˆë ¨ í›„ ì˜ˆë¬¸ ë²ˆì—­\n",
    "    print(\"=== Translation for Test Sentences ===\")\n",
    "    test_sentences = [\"ì˜¤ë°”ë§ˆëŠ” ëŒ€í†µë ¹ì´ë‹¤.\", \"ì‹œë¯¼ë“¤ì€ ë„ì‹œ ì†ì— ì‚°ë‹¤.\", \"ì»¤í”¼ëŠ” í•„ìš” ì—†ë‹¤.\", \"ì¼ê³± ëª…ì˜ ì‚¬ë§ìê°€ ë°œìƒí–ˆë‹¤.\"]\n",
    "    for sentence in test_sentences:\n",
    "        translate(sentence, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36062cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²ˆì—­ ë° Attention ì‹œê°í™”\n",
    "def evaluate(sentence, encoder, decoder):\n",
    "    attention = np.zeros((max_dec_len, max_enc_len))  # max_dec_lenê³¼ max_enc_len ì •ì˜ í•„ìš”\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = enc_tokenizer.texts_to_sequences([sentence.split()])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=max_enc_len,\n",
    "                                                           padding='post')\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    enc_out = encoder(inputs)\n",
    "\n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    dec_input = tf.expand_dims([dec_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_dec_len):  # max_dec_len ì •ì˜ í•„ìš”\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "\n",
    "        result += dec_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if dec_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention\n",
    "\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def translate(sentence, encoder, decoder):\n",
    "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention = attention[:len(result.split()), :len(sentence.split())]\n",
    "    plot_attention(attention, sentence.split(), result.split(' '))\n",
    "\n",
    "# ë²ˆì—­ ì˜ˆì‹œ ì‹¤í–‰\n",
    "translate(\"ì˜¤ë°”ë§ˆëŠ” ëŒ€í†µë ¹ì´ë‹¤.\", encoder, decoder)\n",
    "translate(\"ì‹œë¯¼ë“¤ì€ ë„ì‹œ ì†ì— ì‚°ë‹¤.\", encoder, decoder)\n",
    "translate(\"ì»¤í”¼ëŠ” í•„ìš” ì—†ë‹¤.\", encoder, decoder)\n",
    "translate(\"ì¼ê³± ëª…ì˜ ì‚¬ë§ìê°€ ë°œìƒí–ˆë‹¤.\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9b58e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d17b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a6a3f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe8e94d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
