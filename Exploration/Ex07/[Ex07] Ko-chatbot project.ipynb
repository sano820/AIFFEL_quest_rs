{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25c63e2a",
   "metadata": {},
   "source": [
    "# [Ex07] 한국어 데이터로 챗봇 만들기 프로젝트\n",
    "  \n",
    "- [ ] 한국어 전처리를 통해 학습 데이터셋을 구축  \n",
    "- [ ] 트랜스포머 모델을 구현하여 한국어 챗봇 모델 학습을 정상적으로 진행  \n",
    "- [ ] 한국어 입력 문장에 대해 한국어로 답변하는 함수를 구현  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb0645f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f3c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf47df9c",
   "metadata": {},
   "source": [
    "## 1. 데이터 수집\n",
    "- https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "280dc039",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_path = './data/data.csv'\n",
    " \n",
    "data = pd.read_csv(data_path, encoding=\"cp949\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9753f744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11823 entries, 0 to 11822\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Q       11823 non-null  object\n",
      " 1   A       11823 non-null  object\n",
      " 2   label   11823 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 277.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "760b47fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30728e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Q_length'] = data['Q'].apply(lambda x: len(x.split())) # Q열의 문장 길이\n",
    "data['A_length'] = data['A'].apply(lambda x: len(x.split())) # A열의 문장 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf9d193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmoAAAGDCAYAAACbcTyoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAvHUlEQVR4nO3de5xdVX3//9eba1AQECkC4StgEQwIkURARA1FLuIFRKVYULzUeAEv3583RKsRi7XVIuLXS6FQwVIQgggqLaJAFZFblDsiQUMJBIjcI4ghfH5/nD3pYTKTTGDOzJ6Z1/PxOI/ZZ+211177nDmZd9be+6xUFZIkSWqfVUa7A5IkSRqYQU2SJKmlDGqSJEktZVCTJElqKYOaJElSSxnUJEmSWsqgJknDIMnmSSrJasPY5sFJfjyM7d2QZEazPCvJvw9j20cm+dfhak9Sh0FNGoeS7Jbk0iQPJrkvyS+SvGQY2n17kkuGo4/DKcm8JK8aS/tM8u0kf07ycPO4Psk/JFm3r05VnVpVew2xrb9fUb2q2raqLn6qfe7a34wk8/u1/YWq+tun27akJzOoSeNMkmcBPwS+Bjwb2BT4HPDYaPZLA/qnqloH2BB4B7AL8IskzxzOnQznKJ+kkWVQk8afFwBU1WlVtaSqHq2qH1fVtX0VkrwzyU1J7k9yfpLnda2rJO9NckuSB5J8PR0vBL4FvDTJoiQPNPXXTPLlJP+T5O4k30qyVrNuRpL5ST6S5J4kC5K8o2tfayX55yS3NaN/l3Rtu0szKvhAkmv6TtmtjCSrJDkiya1J7k1yRpJnN+v6TlUe2vT9D0k+1a9vJzev0U1JPt43ipTkO8D/AX7QvBYf79rtwQO1tzxV9aequhJ4PbABndD2pBHM5j34SvM6PpTkuiTbJZkJHAx8vOnLD5r685J8Ism1wB+TrDbAKOCkJN9tRvR+lWSHruOvJH/Z9fzbSf6+CZH/CWzS7G9Rkk3S71Rqktenc6r1gSQXN78/fevmJflokmub9/27SSYN5bWSJhqDmjT+/BZY0oSMVydZv3tlkv2AI4ED6Izk/Bw4rV8brwVeAmwPHAjsXVU3Ae8FfllVa1fVek3dL9IJh1OBv6QzgveZrraeC6zblL8L+HpXn74MTAN2pTP693HgiSSbAj8C/r4p/yhwVpINV/K1+ACwP/BKYBPgfuDr/ersBmwN7AF8pitQfBbYHNgS2BM4pG+Dqnor8D/A65rX4p+G0N4KVdXDwAXAywdYvRfwCjqv9bp03pd7q+p44FQ6o3NrV9XrurZ5C/AaYL2qenyANvcDzqTzGv8H8P0kq6+gj38EXg3c2exv7aq6s7tOkhfQ+Z36MJ3fsfPohNo1uqodCOwDbEHn9+zty9uvNFEZ1KRxpqoeohMWCjgBWJjk3CQbNVXeC/xDVd3U/PH+AjC1e1QN+GJVPVBV/wNcRCeELSNJgJnA/62q+5qg8QXgoK5qi4GjqmpxVZ0HLAK2TrIK8E7gQ1V1RzP6d2lVPUYnFJ1XVedV1RNVdQFwFbDvSr4c7wU+VVXzm3ZnAW/Kk08Ffq4ZdbwGuAboG1U6EPhCVd1fVfOB44a4z8HaG6o76QSn/hYD6wDbAGnevwUraOu4qrq9qh4dZP2cqppdVYuBY4BJdE6/Pl1/Dfyoqi5o2v4ysBadQN7dtzur6j7gBwzyOyZNdAY1aRxq/oi/vaomA9vRGU06tln9POCrzSmpB4D7gNAZ8epzV9fyI8Dag+xqQ+AZwJyu9v6rKe9zb7/RnL72nkMnGNw6QLvPA97c12bT7m7Axss77kHaOburjZuAJcBGXXUGO9ZNgNu71nUvL89QX7vBbErnPXmSqroQ+H90RgTvSXJ8OtcjLs+K+rx0fVU9Acync9xP1ybAbf3avp2n9jsmTWgGNWmcq6rfAN+mE9ig8wfzPVW1Xtdjraq6dCjN9Xv+B+BRYNuuttatqqH80f0D8Cfg+QOsux34Tr8+PrOqvjiEdvu38+p+7UyqqjuGsO0CYHLX8836re//WjxtSdYGXkXndPQyquq4qpoGTKFzCvRjK+jLivq49JiaEc7JdEb0oBOentFV97kr0e6ddEJyX9tp9jWU111SF4OaNM4k2aa5eH9y83wzOtcqXdZU+RbwySTbNuvXTfLmITZ/NzC571qjZqTkBOArSf6iaW/TJHuvqKFm25OAY5qL0VdN8tIkawL/Drwuyd5N+aR0bkyYvJwmV2/q9T1Wa4716L7Tukk2bK7RG4oz6LxO6zfXzB0+wGux5RDbWq50bsiYBnyfznV0/zZAnZck2bm5huyPdELuE0+zL9OSHNC8Vh+mc2dw3+/J1cDfNK//PnSu8+tzN7BBur5KpJ8zgNck2aPp70eatofynwFJXQxq0vjzMLAzcHmSP9L5w3s9nT+WVNXZwD8Cpyd5qFn36iG2fSFwA3BXkj80ZZ8A5gKXNe39hM7F9EPxUeA64Eo6p/v+EVilqm6nc6H7kcBCOiNjH2P5/2adR2d0r+8xC/gqcC7w4yQP03ktdh5i346icyrw980xzebJX3HyD8Cnm9OqHx1im/19vOnXvcApwBxg1+aC/f6eRScU30/ntOK9wJeadScCU5q+fH8l9n8OnevJ7gfeChzQXFMG8CHgdcADdO4qXdpuM0p7GvC7Zp9POl1aVTfTuc7wa3RGTl9H58aLP69E3yTRuSB1tPsgSa2X5H3AQVX1yhVWlqRh4oiaJA0gycZJXpbOd7FtTWdE8uzR7pekicVvq5akga0B/Aud7/l6ADgd+MZodkjSxNOzEbXmYt4r0vlG8RuSfK4p/3aS3ye5unlMbcqT5Lgkc5tvq96xq61D0/mW9FuSHNqrPktSn6q6raq2a+423bSqPuI1VpJGWi9H1B4D/qqqFjV3/VyS5D+bdR+rqtn96r8a2Kp57Ax8E9g5nelePgtMp3NL+Jwk51bV/T3suyRJ0qjr2YhadSxqnq7ePJZ358J+wCnNdpcB6yXZGNgbuKD51vP76Uyvsk+v+i1JktQWPb1GLcmqdG43/0vg61V1eXPn1NFJPgP8FDiimdplU578Ldrzm7LByvvvayadqWx45jOfOW2bbbbpwRFJkiQNrzlz5vyhqgacy7inQa2qltCZQ3A9OtO4bAd8ks7UIWsAx9P5DqajhmFfxzftMX369LrqqquebpOSJEk9l+S2wdaNyNdzVNUDdCZ23qeqFjSnNx+j8+3bOzXV7uDJU7RMbsoGK5ckSRrXennX54bNSBpJ1gL2BH7TXHfWN/fb/nS+FR063x7+tubuz12AB6tqAXA+sFczjcv6wF5NmSRJ0rjWy1OfGwMnN9eprQKcUVU/THJhkg2B0JlL7r1N/fOAfelMRfMI8A6AqrovyefpTDEDcFRV3dfDfkuSJLXCuJxCymvUJEl6ehYvXsz8+fP505/+NNpdGTcmTZrE5MmTWX311Z9UnmROVU0faBtnJpAkScuYP38+66yzDptvvjmdq5X0dFQV9957L/Pnz2eLLbYY8nbO9SlJkpbxpz/9iQ022MCQNkySsMEGG6z0CKVBTZIkDciQNryeyutpUJMkSa00f/589ttvP7baaiu23HJLDj/8cB577LFl6s2bN4/tttuup335whe+MKL76+M1apIkaYVmzRrZ9qqKAw44gPe9732cc845LFmyhJkzZ/Lxj3+cr371q8PbmSH4whe+wJFHHjni+3VETZIktc6FF17IpEmTeMc73gHAqquuyle+8hVOOeUUFi1atIKtO+bMmcMrX/lKpk2bxt57782CBQsAmDFjBp/4xCfYaaedeMELXsDPf/5zAB555BEOPPBApkyZwhve8AZ23nlnrrrqKo444ggeffRRpk6dysEHHwzAkiVLePe73822227LXnvtxaOPPgrAcccdx5QpU9h+++056KCDnvbrYFCTJEmtc8MNNzBt2rQnlT3rWc9i8803Z+7cuSvcfvHixXzgAx9g9uzZzJkzh3e+85186lOfWrr+8ccf54orruDYY4/lc5/7HADf+MY3WH/99bnxxhv5/Oc/z5w5cwD44he/yFprrcXVV1/NqaeeCsAtt9zCYYcdxg033MB6663HWWedtbTur3/9a6699lq+9a1vPe3XwVOfkiRp3Ln55pu5/vrr2XPPPYHOCNjGG2+8dP0BBxwAwLRp05g3bx4Al1xyCR/60IcA2G677dh+++0HbX+LLbZg6tSpy7Sx/fbbc/DBB7P//vuz//77P+3jMKhJkqTWmTJlCrNnz35S2UMPPcRdd93F1ltvvcLtq4ptt92WX/7ylwOuX3PNNYHOKdXHH398pfvXt31fG32nPn/0ox/xs5/9jB/84AccffTRXHfdday22lOPWwY1LfeCzuG+eFSSpKHYY489OOKIIzjllFN429vexpIlS/jIRz7C4YcfzlprrbXC7bfeemsWLlzIL3/5S1760peyePFifvvb37LtttsOus3LXvYyzjjjDHbffXduvPFGrrvuuqXrVl99dRYvXrzMrALdnnjiCW6//XZ23313dtttN04//XQWLVrEeuutt1LH3s1r1CRJUusk4eyzz2b27NlstdVWbLDBBqyyyipPus6s280338zkyZOXPs455xxmz57NJz7xCXbYYQemTp3KpZdeutx9vv/972fhwoVMmTKFT3/602y77basu+66AMycOXPpac3BLFmyhEMOOYQXvehFvPjFL+aDH/zg0wpp4FyfwhE1SdKybrrpJl74wheOdjeWuvTSS3nLW97C2WefzY477tiTfSxZsoTFixczadIkbr31Vl71qldx8803s8YaawzbPgZ6XZ3rU5IkjWm77rort912W0/38cgjj7D77ruzePFiqopvfOMbwxrSngqDmiRJErDOOuvQtjNyXqMmSZLUUgY1SZKkljKoSZIktZRBTZIkqaUMapIkqbW+//3vk4Tf/OY3g9ZZe+21e9qHY489lkceeWTE9tfNuz4lSdKKDfcXaw6xvdNOO43ddtuN0047benk6SPt2GOP5ZBDDuEZz3jGiO/bETVJktRKixYt4pJLLuHEE0/k9NNPX6ltb731VvbZZx+mTZvGy1/+8qUjcm9/+9v54Ac/yK677sqWW265dD7RJ554gve///1ss8027Lnnnuy7777Mnj2b4447jjvvvJPdd9+d3XfffWn7n/rUp9hhhx3YZZdduPvuuwE488wz2W677dhhhx14xSteMSyvgUFNkiS10jnnnMM+++zDC17wAjbYYAPmzJkz5G1nzpzJ1772NebMmcOXv/xl3v/+9y9dt2DBAi655BJ++MMfcsQRRwDwve99j3nz5nHjjTfyne98Z+lk7h/84AfZZJNNuOiii7jooosA+OMf/8guu+zCNddcwyte8QpOOOEEAI466ijOP/98rrnmGs4999xheQ0MapIkqZVOO+00DjroIAAOOuggTjvttCFtt2jRIi699FLe/OY3M3XqVN7znvewYMGCpev3339/VlllFaZMmbJ0NOySSy7hzW9+M6ussgrPfe5znzR61t8aa6zBa1/7WgCmTZvGvHnzgM6k7m9/+9s54YQTWLJkyVM55GV4jZokSWqd++67jwsvvJDrrruOJCxZsoQkfOlLXyLJcrd94oknWG+99bj66qsHXL/mmmsuXX4qc56vvvrqS/uw6qqr8vjjjwPwrW99i8svv5wf/ehHTJs2jTlz5rDBBhusdPvdHFGTJEmtM3v2bN761rdy2223MW/ePG6//Xa22GILfv7zn69w22c961lsscUWnHnmmUAnjF1zzTXL3eZlL3sZZ511Fk888QR33303F1988dJ166yzDg8//PAK93vrrbey8847c9RRR7Hhhhty++23r3CbFTGoSZKk1jnttNN4wxve8KSyN77xjQOe/nzkkUeYPHny0scxxxzDqaeeyoknnsgOO+zAtttuyznnnLPc/b3xjW9k8uTJTJkyhUMOOYQdd9yRddddF+hc77bPPvss93QowMc+9jFe9KIXsd1227Hrrruyww47rORRLytPZciv7aZPn15tm1S1zZZ3h/Rw340tSRobbrrpJl74wheOdjdG1KJFi1h77bW599572WmnnfjFL37Bc5/73GHdx0Cva5I5VTV9oPpeoyZJkgS89rWv5YEHHuDPf/4zf/d3fzfsIe2pMKhJkiTBk65LawuvUZMkSWopR9QEwIyLZw28on+xF61J0oRRVSv8KgwN3VO5L8ARNUmStIxJkyZx7733PqVwoWVVFffeey+TJk1aqe0cUZMkScuYPHky8+fPZ+HChaPdlXFj0qRJTJ48eaW2MahJkqRlrL766myxxRaj3Y0Jz1OfkiRJLWVQkyRJaimDmiRJUksZ1CRJklqqZ0EtyaQkVyS5JskNST7XlG+R5PIkc5N8N8kaTfmazfO5zfrNu9r6ZFN+c5K9e9VnSZKkNunliNpjwF9V1Q7AVGCfJLsA/wh8par+ErgfeFdT/13A/U35V5p6JJkCHARsC+wDfCPJqj3styRJUiv0LKhVx6Lm6erNo4C/AmY35ScD+zfL+zXPadbvkc7XIe8HnF5Vj1XV74G5wE696rckSVJb9PQatSSrJrkauAe4ALgVeKCqHm+qzAc2bZY3BW4HaNY/CGzQXT7ANt37mpnkqiRX+eV8kiRpPOhpUKuqJVU1FZhMZxRsmx7u6/iqml5V0zfccMNe7UaSJGnEjMhdn1X1AHAR8FJgvSR9MyJMBu5olu8ANgNo1q8L3NtdPsA2kiRJ41Yv7/rcMMl6zfJawJ7ATXQC25uaaocC5zTL5zbPadZfWJ2ZYM8FDmruCt0C2Aq4olf9liRJaotezvW5MXByc4fmKsAZVfXDJDcCpyf5e+DXwIlN/ROB7ySZC9xH505PquqGJGcANwKPA4dV1ZIe9luSJKkVehbUqupa4MUDlP+OAe7arKo/AW8epK2jgaOHu4+SJElt5swEkiRJLWVQkyRJaimDmiRJUksZ1CRJklrKoCZJktRSBjVJkqSWMqhJkiS1lEFNkiSppQxqkiRJLWVQkyRJaimDmiRJUksZ1CRJklrKoCZJktRSBjVJkqSWMqhJkiS1lEFNkiSppQxqkiRJLWVQkyRJaimDmiRJUksZ1CRJklrKoCZJktRSBjVJkqSWMqhJkiS1lEFNkiSppQxqkiRJLWVQkyRJaimDmiRJUksZ1CRJklrKoCZJktRSBjVJkqSWMqhJkiS1lEFNkiSppQxqkiRJLWVQkyRJaimDmiRJUksZ1CRJklrKoCZJktRSPQtqSTZLclGSG5PckORDTfmsJHckubp57Nu1zSeTzE1yc5K9u8r3acrmJjmiV32WJElqk9V62PbjwEeq6ldJ1gHmJLmgWfeVqvpyd+UkU4CDgG2BTYCfJHlBs/rrwJ7AfODKJOdW1Y097LskSdKo61lQq6oFwIJm+eEkNwGbLmeT/YDTq+ox4PdJ5gI7NevmVtXvAJKc3tQ1qLXYrFlPbZ0kSfpfI3KNWpLNgRcDlzdFhye5NslJSdZvyjYFbu/abH5TNlh5/33MTHJVkqsWLlw43IcgSZI04np56hOAJGsDZwEfrqqHknwT+DxQzc9/Bt75dPdTVccDxwNMnz69nm57o8WRKEmS1KenQS3J6nRC2qlV9T2Aqrq7a/0JwA+bp3cAm3VtPrkpYznlkiRJ41bPglqSACcCN1XVMV3lGzfXrwG8Abi+WT4X+I8kx9C5mWAr4AogwFZJtqAT0A4C/qZX/R5Vs2Yx4+Llrf/fepIkafzr5Yjay4C3AtclubopOxJ4S5KpdE59zgPeA1BVNyQ5g85NAo8Dh1XVEoAkhwPnA6sCJ1XVDT3styRJUiv08q7PS+iMhvV33nK2ORo4eoDy85a3nSRJ0njkzASSJEktZVCTJElqKYOaJElSSxnUJEmSWsqgJkmS1FI9n5lAE9OMi2cNvrJ7ld8JJ0nSoBxRkyRJaimDmiRJUksZ1CRJklrKoCZJktRSBjVJkqSWMqhJkiS1lEFNkiSppQxqkiRJLWVQkyRJaimDmiRJUksZ1CRJklrKoCZJktRSBjVJkqSWMqhJkiS1lEFNkiSppQxqkiRJLWVQkyRJaimDmiRJUksZ1CRJklrKoCZJktRSBjVJkqSWMqhJkiS1lEFNkiSppQxqkiRJLWVQkyRJaimDmiRJUksZ1CRJklrKoCZJktRSBjVJkqSWMqhJkiS1lEFNkiSppXoW1JJsluSiJDcmuSHJh5ryZye5IMktzc/1m/IkOS7J3CTXJtmxq61Dm/q3JDm0V32WJElqk16OqD0OfKSqpgC7AIclmQIcAfy0qrYCfto8B3g1sFXzmAl8EzrBDvgssDOwE/DZvnAnSZI0nvUsqFXVgqr6VbP8MHATsCmwH3ByU+1kYP9meT/glOq4DFgvycbA3sAFVXVfVd0PXADs06t+S5IktcWQglqSFz2dnSTZHHgxcDmwUVUtaFbdBWzULG8K3N612fymbLBySZKkcW2oI2rfSHJFkvcnWXdldpBkbeAs4MNV9VD3uqoqoFamveXsZ2aSq5JctXDhwuFoUpIkaVQNKahV1cuBg4HNgDlJ/iPJnivaLsnqdELaqVX1vab47uaUJs3Pe5ryO5r2+0xuygYr79/H46tqelVN33DDDYdyWJIkSa025GvUquoW4NPAJ4BXAscl+U2SAwaqnyTAicBNVXVM16pzgb47Nw8Fzukqf1tz9+cuwIPNKdLzgb2SrN/cRLBXUyZJkjSurTaUSkm2B94BvIbOxfyvq6pfJdkE+CXwvQE2exnwVuC6JFc3ZUcCXwTOSPIu4DbgwGbdecC+wFzgkWZ/VNV9ST4PXNnUO6qq7luZg5QkSRqLhhTUgK8B/wocWVWP9hVW1Z1JPj3QBlV1CZBB2ttjgPoFHDZIWycBJw2xr5IkSePCUIPaa4BHq2oJQJJVgElV9UhVfadnvZMkSZrAhnqN2k+AtbqeP6MpkyRJUo8MNahNqqpFfU+a5Wf0pkuSJEmCoQe1P/abe3Ma8Ohy6kuSJOlpGuo1ah8GzkxyJ50bBJ4L/HWvOiVJkqQhBrWqujLJNsDWTdHNVbW4d92SJEnSUEfUAF4CbN5ss2MSquqUnvRKkiRJQ/7C2+8AzweuBpY0xQUY1CRJknpkqCNq04EpzZfSSpIkaQQM9a7P6+ncQCBJkqQRMtQRtecANya5Anisr7CqXt+TXkmSJGnIQW1WLzshSZKkZQ316zn+O8nzgK2q6idJngGs2tuuSZIkTWxDukYtybuB2cC/NEWbAt/vUZ8kSZLE0G8mOAx4GfAQQFXdAvxFrzolSZKkoQe1x6rqz31PkqxG53vUJEmS1CNDDWr/neRIYK0kewJnAj/oXbckSZI01KB2BLAQuA54D3Ae8OledUqSJElDv+vzCeCE5iFJkqQRMNS5Pn/PANekVdWWw94jSZIkASs312efScCbgWcPf3ckSZLUZ0jXqFXVvV2PO6rqWOA1ve2aJEnSxDbUU587dj1dhc4I21BH4yRJkvQUDDVs/XPX8uPAPODAYe+NJEmSlhrqXZ+797ojkiRJerKhnvr8/5a3vqqOGZ7uSJIkqc/K3PX5EuDc5vnrgCuAW3rRKUmSJA09qE0GdqyqhwGSzAJ+VFWH9KpjkiRJE91Qp5DaCPhz1/M/N2WSJEnqkaGOqJ0CXJHk7Ob5/sDJPemRJEmSgKHf9Xl0kv8EXt4UvaOqft27bkmSJGmopz4BngE8VFVfBeYn2aJHfZIkSRJDDGpJPgt8AvhkU7Q68O+96pQkSZKGPqL2BuD1wB8BqupOYJ1edUqSJElDD2p/rqoCCiDJM3vXJUmSJMHQg9oZSf4FWC/Ju4GfACf0rluSJEla4V2fSQJ8F9gGeAjYGvhMVV3Q475JkiRNaCsMalVVSc6rqhcBhjNJkqQRMtRTn79K8pKVaTjJSUnuSXJ9V9msJHckubp57Nu17pNJ5ia5OcneXeX7NGVzkxyxMn2QJEkay4Y6M8HOwCFJ5tG58zN0Btu2X8423wb+H51ZDbp9paq+3F2QZApwELAtsAnwkyQvaFZ/HdgTmA9cmeTcqrpxiP2WJEkas5Yb1JL8n6r6H2Dv5dUbSFX9LMnmQ6y+H3B6VT0G/D7JXGCnZt3cqvpd05/Tm7oGNUmSNO6t6NTn9wGq6jbgmKq6rfvxFPd5eJJrm1Oj6zdlmwK3d9WZ35QNVr6MJDOTXJXkqoULFz7FrkmSJLXHioJaupa3HIb9fRN4PjAVWAD88zC0CUBVHV9V06tq+oYbbjhczUqSJI2aFV2jVoMsPyVVdXffcpITgB82T+8ANuuqOrkpYznlkiRJ49qKRtR2SPJQkoeB7Zvlh5I8nOShld1Zko27nr4B6Lsj9FzgoCRrNpO9bwVcAVwJbJVkiyRr0Lnh4NyV3a8kSdJYtNwRtapa9ak2nOQ0YAbwnCTzgc8CM5JMpTM6Nw94T7OfG5KcQecmgceBw6pqSdPO4cD5wKrASVV1w1PtkyRJ0lgy1K/nWGlV9ZYBik9cTv2jgaMHKD8POG8Yu6ZxaNasp7ZOkqQ261lQk0bDjItnLVs4QJHpTZI0Fgx1ZgJJkiSNMIOaJElSSxnUJEmSWsqgJkmS1FIGNUmSpJYyqEmSJLWUQU2SJKmlDGqSJEktZVCTJElqKYOaJElSSxnUJEmSWsqgJkmS1FIGNUmSpJYyqEmSJLWUQU2SJKmlDGqSJEktZVCTJElqKYOaJElSSxnUJEmSWsqgJkmS1FIGNUmSpJYyqEmSJLWUQU2SJKmlDGqSJEktZVCTJElqKYOaJElSSxnUJEmSWsqgJkmS1FIGNUmSpJYyqEmSJLWUQU2SJKmlDGqSJEktZVCTJElqKYOaJElSSxnUJEmSWsqgJkmS1FI9C2pJTkpyT5Lru8qeneSCJLc0P9dvypPkuCRzk1ybZMeubQ5t6t+S5NBe9VeSJKltejmi9m1gn35lRwA/raqtgJ82zwFeDWzVPGYC34ROsAM+C+wM7AR8ti/cSZIkjXc9C2pV9TPgvn7F+wEnN8snA/t3lZ9SHZcB6yXZGNgbuKCq7quq+4ELWDb8SZIkjUsjfY3aRlW1oFm+C9ioWd4UuL2r3vymbLBySZKkcW/UbiaoqgJquNpLMjPJVUmuWrhw4XA1K0mSNGpGOqjd3ZzSpPl5T1N+B7BZV73JTdlg5cuoquOranpVTd9www2HveOSJEkjbaSD2rlA352bhwLndJW/rbn7cxfgweYU6fnAXknWb24i2KspkyRJGvdW61XDSU4DZgDPSTKfzt2bXwTOSPIu4DbgwKb6ecC+wFzgEeAdAFV1X5LPA1c29Y6qqv43KEiSJI1LPQtqVfWWQVbtMUDdAg4bpJ2TgJOGsWuSJEljgjMTSJIktZRBTZIkqaUMapIkSS1lUJMkSWopg5okSVJLGdQkSZJayqAmSZLUUgY1SZKkljKoSZIktZRBTZIkqaUMapIkSS1lUJMkSWqpnk3KLo0bs2YNbz1JkobIETVJkqSWMqhJkiS1lEFNkiSppQxqkiRJLWVQkyRJainv+pSWY9YsmHHxwOtmzBjBjkiSJiSD2tPh1zZIkqQe8tSnJElSSxnUJEmSWsqgJkmS1FIGNUmSpJYyqEmSJLWUQU2SJKmlDGqSJEktZVCTJElqKYOaJElSSxnUJEmSWsoppJ6Giy8efJ3zQEqSpKfLETVJkqSWMqhJkiS1lEFNkiSppQxqkiRJLWVQkyRJaimDmiRJUksZ1CRJklpqVIJaknlJrktydZKrmrJnJ7kgyS3Nz/Wb8iQ5LsncJNcm2XE0+ixJkjTSRnNEbfeqmlpV05vnRwA/raqtgJ82zwFeDWzVPGYC3xzxnkqSJI2CNp363A84uVk+Gdi/q/yU6rgMWC/JxqPQP0mSpBE1WkGtgB8nmZNkZlO2UVUtaJbvAjZqljcFbu/adn5T9iRJZia5KslVCxcu7FW/JUmSRsxozfW5W1XdkeQvgAuS/KZ7ZVVVklqZBqvqeOB4gOnTp6/UtpIkSW00KiNqVXVH8/Me4GxgJ+DuvlOazc97mup3AJt1bT65KZMkSRrXRjyoJXlmknX6loG9gOuBc4FDm2qHAuc0y+cCb2vu/twFeLDrFKkkSdK4NRqnPjcCzk7St///qKr/SnIlcEaSdwG3AQc29c8D9gXmAo8A7xj5LkuSJI28EQ9qVfU7YIcByu8F9higvIDDRqBrkiRJrdKmr+eQJElSF4OaJElSS43W13NIWpFZs4a3niRpzHFETZIkqaUMapIkSS1lUJMkSWopg5okSVJLeTOB1CLd9wXMuPjJ62bMGMGOSJJawRE1SZKkljKoSZIktZRBTZIkqaUMapIkSS1lUJMkSWopg5okSVJLGdQkSZJayqAmSZLUUgY1SZKkljKoSZIktZRBTZIkqaUMapIkSS1lUJMkSWqp1Ua7A5JGyKxZw1tPktRzjqhJkiS1lEFNkiSppQxqkiRJLWVQkyRJailvJpAmsIsvHqBsVuen9xRI0uhzRE2SJKmlDGqSJEktZVCTJElqKYOaJElSSxnUJEmSWsq7PiX1ntNXSdJT4oiaJElSSzmiJmnY9R8Ym3Fx1/KMEeyIJI1xBjVJY4+nUiVNEJ76lCRJaqkxE9SS7JPk5iRzkxwx2v2RJEnqtTFx6jPJqsDXgT2B+cCVSc6tqhtHt2eS2mCgOUuhM2/psJ79XJnGPO0qaRiMiaAG7ATMrarfASQ5HdgPMKhJGtu83k7ScoyVoLYpcHvX8/nAzqPUF0kTwEC5qO/u1V7duTrYyCDAsO9yuAPiaI42jtaxGJ41AlJVo92HFUryJmCfqvrb5vlbgZ2r6vCuOjOBmc3T7YDrR7yj7fEc4A+j3YlRMpGPHSb28XvsE9dEPv6JfOwwfo7/eVW14UArxsqI2h3AZl3PJzdlS1XV8cDxAEmuqqrpI9e9dpnIxz+Rjx0m9vF77BPz2GFiH/9EPnaYGMc/Vu76vBLYKskWSdYADgLOHeU+SZIk9dSYGFGrqseTHA6cD6wKnFRVN4xytyRJknpqTAQ1gKo6DzhviNWP72VfxoCJfPwT+dhhYh+/xz5xTeTjn8jHDhPg+MfEzQSSJEkT0Vi5Rk2SJGnCGdNBbUXTSiVZM8l3m/WXJ9l8FLrZE0k2S3JRkhuT3JDkQwPUmZHkwSRXN4/PjEZfeyHJvCTXNcd11QDrk+S45r2/NsmOo9HP4ZZk66738+okDyX5cL864+p9T3JSknuSXN9V9uwkFyS5pfm5/iDbHtrUuSXJoSPX6+ExyLF/Kclvmt/rs5OsN8i2y/2MjAWDHP+sJHd0/X7vO8i2Y3rawUGO/btdxz0vydWDbDum3/vB/r5NlM/9MqpqTD7o3FRwK7AlsAZwDTClX533A99qlg8Cvjva/R7G498Y2LFZXgf47QDHPwP44Wj3tUfHPw94znLW7wv8JxBgF+Dy0e5zD16DVYG76Hz/zrh934FXADsC13eV/RNwRLN8BPCPA2z3bOB3zc/1m+X1R/t4huHY9wJWa5b/caBjb9Yt9zMyFh6DHP8s4KMr2G6Ffx/a/hjo2Put/2fgM+PxvR/s79tE+dz3f4zlEbWl00pV1Z+Bvmmluu0HnNwszwb2SJIR7GPPVNWCqvpVs/wwcBOdGRzUsR9wSnVcBqyXZOPR7tQw2wO4tapuG+2O9FJV/Qy4r19x92f7ZGD/ATbdG7igqu6rqvuBC4B9etXPXhjo2Kvqx1X1ePP0MjrfKzkuDfLeD8VQ/j602vKOvfk7diBw2oh2aoQs5+/bhPjc9zeWg9pA00r1DypL6zT/sD0IbDAivRtBzSndFwOXD7D6pUmuSfKfSbYd2Z71VAE/TjInnVkp+hvK78dYdxCD/0M9Xt/3PhtV1YJm+S5gowHqTITfgXfSGTkeyIo+I2PZ4c2p35MGOf013t/7lwN3V9Utg6wfN+99v79vE/JzP5aDmoAkawNnAR+uqof6rf4VndNiOwBfA74/wt3rpd2qakfg1cBhSV4x2h0aSel88fPrgTMHWD2e3/dlVOd8x4S7fT3Jp4DHgVMHqTJePyPfBJ4PTAUW0DkFONG8heWPpo2L9355f98m0ud+LAe1FU4r1V0nyWrAusC9I9K7EZBkdTq/xKdW1ff6r6+qh6pqUbN8HrB6kueMcDd7oqruaH7eA5xN51RHt6H8foxlrwZ+VVV3918xnt/3Lnf3ncpuft4zQJ1x+zuQ5O3Aa4GDmz9YyxjCZ2RMqqq7q2pJVT0BnMDAxzWe3/vVgAOA7w5WZzy894P8fZuQn/uxHNSGMq3UuUDfHR9vAi4c7B+1saa5RuFE4KaqOmaQOs/tuyYvyU503u8xH1STPDPJOn3LdC6uvr5ftXOBt6VjF+DBriHz8WDQ/1GP1/e9n+7P9qHAOQPUOR/YK8n6zemxvZqyMS3JPsDHgddX1SOD1BnKZ2RM6net6RsY+LjG87SDrwJ+U1XzB1o5Ht775fx9m5if+9G+m+HpPOjc2fdbOnf3fKopO4rOP2AAk+icGpoLXAFsOdp9HsZj343OsO+1wNXNY1/gvcB7mzqHAzfQuePpMmDX0e73MB37ls0xXdMcX997333sAb7e/G5cB0wf7X4P4/E/k07wWrerbNy+73QC6QJgMZ3rTd5F51rTnwK3AD8Bnt3UnQ78a9e272w+/3OBd4z2sQzTsc+lcw1O3+e+7872TYDzmuUBPyNj7THI8X+n+UxfS+cP98b9j795vszfh7H0GOjYm/Jv933Wu+qOq/d+OX/fJsTnvv/DmQkkSZJaaiyf+pQkSRrXDGqSJEktZVCTJElqKYOaJElSSxnUJEmSWsqgJqnnkizqcfsfTvKM4dhfkjWT/CTJ1Un+ut+6XZJc3qy7Kcmsp7GfI5/qtpImDr+eQ1LPJVlUVWv3sP15dL4r7w9Pd3/NFyT/fVW9aoB1NwMHVtU1SVYFtq6qG5/ifnr6mkgaHxxRkzQqkjw/yX81E0f/PMk2Tfm3kxyX5NIkv0vypqZ8lSTfSPKbJBckOS/Jm5J8kM4Xfl6U5KKu9o9uJqa/LMkykzcneXaS7zeTe1+WZPskfwH8O/CSZtTs+f02+ws6X0JKdaYxurFp65nNBOFXJPl1kv2a8rcn+V5znLck+aem/IvAWs0+Tm3KDmm2vzrJvzRBkCSLBjqWJBslObspvybJroO10zy+neT6JNcl+b/D9DZK6jGDmqTRcjzwgaqaBnwU+EbXuo3pfDv5a4EvNmUHAJsDU4C3Ai8FqKrjgDuB3atq96buM4HLqjMx/c+Adw+w/88Bv66q7YEjgVOqMzfi3wI/r6qpVXVrv22+AtzcBKT3JJnUlH+KzhR1OwG7A19qpu+BzuThfw28CPjrJJtV1RHAo80+Dk7ywqbOy6pqKrAEOHgFx3Ic8N9N+Y7ADctpZyqwaVVtV1UvAv5tgNdDUgutNtodkDTxJFkb2BU4s5mWFGDNrirfr86k2zd2jYbtBpzZlN/VPXo2gD8DP2yW5wB7DlBnN+CNAFV1YZINkjxref2uqqOaEbC9gL+hM+fqjOb565N8tKk6Cfg/zfJPq+rB5rhvBJ5HZwqobnsA04Arm9djLf53wunBjuWvgLc1/VoCPJjkrYO08wNgyyRfA34E/Hh5xympPQxqkkbDKsADzajPQB7rWs4gdZZncf3vBbhLGMZ/65pRtm8mOQFYmGSDpo9vrKqbu+sm2ZknH8tgfQlwclV9coB1K3Msg7aTZAdgbzrzwh5IZz5ESS3nqU9JI66qHgJ+n+TNAOnYYQWb/QJ4Y3Ot2kZ0RrL6PAyss5Ld+DnN6cUkM4A/NP0aVJLX5H+HALeiE5weAM4HPtC3LsmLh7D/xUlWb5Z/CrypuUau7/q5561g+58C72vqr5pk3cHaSfIcYJWqOgv4NJ1TpZLGAEfUJI2EZySZ3/X8GDoh6ZtJPg2sDpwOXLOcNs6ic4rwRjqnDn8FPNisOx74ryR3dl2ntiKzgJOSXAs8Ahw6hG3eCnwlySPA48DBVbUkyeeBY4Frk6wC/J7O9XXLc3xT/1fNdWqfBn7cbL8YOAy4bTnbfwg4Psm76ATG91XVLwdp51Hg35oygIFG7iS1kF/PIWnMSLJ2VS1qTjdeQeei+btGu1+S1CuOqEkaS36YZD1gDeDzhjRJ450japIkSS3lzQSSJEktZVCTJElqKYOaJElSSxnUJEmSWsqgJkmS1FIGNUmSpJb6/wGdvWiZRN0jJQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(data[\"Q_length\"], bins=50, alpha=0.5, color=\"blue\", label=\"Q Lengths\")\n",
    "plt.hist(data[\"A_length\"], bins=50, alpha=0.5, color=\"red\", label=\"A Lengths\")\n",
    "\n",
    "plt.title(\"Sentence Length Distribution\")\n",
    "plt.xlabel(\"Length of Sentences\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6a8364",
   "metadata": {},
   "source": [
    "- Q와 A 모두 3~4 정도 길이가 가장 많다.  \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b821882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5290\n",
      "1    3570\n",
      "2    2963\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# label 별 갯수 보기\n",
    "print(data['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edbc5b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11823\n"
     ]
    }
   ],
   "source": [
    "# 전체 데이터 개수\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbf586d",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4284bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q           0\n",
      "A           0\n",
      "label       0\n",
      "Q_length    0\n",
      "A_length    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 각 열의 결측치 개수 확인\n",
    "missing_data = data.isnull().sum()\n",
    "print(missing_data)\n",
    "\n",
    "# 결측값 제거\n",
    "# data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "066f5b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복된 데이터 제거\n",
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "142ff8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11823\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdefe53",
   "metadata": {},
   "source": [
    "- 중복이랑 결측 x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "542387a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    \n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)   # 각종 구두점과의 거리 만들기\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)  # 공백이 두칸이면 한칸으로..\n",
    "    sentence = re.sub(r'[^a-zA-Z0-9.,?!가-힣]', ' ', sentence) # 한글과 영어, 구두점을 제외한 모든 문자 공백으로 대체\n",
    "    \n",
    "    sentence = sentence.strip() \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1890dd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = data['Q'].apply(preprocess_sentence).tolist()  # 전처리 결과를 각각 리스트로 변환하여 questions와 answers에 저장\n",
    "answers = data['A'].apply(preprocess_sentence).tolist()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "031e866d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후의 22번째 질문 샘플: 가스비 장난 아님\n",
      "전처리 후의 22번째 답변 샘플: 다음 달에는 더 절약해봐요 .\n"
     ]
    }
   ],
   "source": [
    "print('전처리 후의 22번째 질문 샘플: {}'.format(questions[21]))\n",
    "print('전처리 후의 22번째 답변 샘플: {}'.format(answers[21]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e489a6",
   "metadata": {},
   "source": [
    "---\n",
    "## 3 SubwordTextEncoder 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64304e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e052756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29cf3116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [8164]\n",
      "END_TOKEN의 번호 : [8165]\n"
     ]
    }
   ],
   "source": [
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddd087b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8166\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38a0fffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 21번째 질문 샘플: [5758, 610, 2487, 4159]\n",
      "정수 인코딩 후의 21번째 답변 샘플: [2355, 7504, 7, 6268, 97, 1]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
    "# 각 토큰을 고유한 정수로 변환\n",
    "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(questions[21])))\n",
    "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(answers[21])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a9a9b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n",
    "MAX_LENGTH = 10\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0adef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "        # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "        # 최대 길이 10 이하인 경우에만 데이터셋으로 허용\n",
    "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "  \n",
    "    # 최대 길이 10 으로 모든 데이터셋을 패딩\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "    return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31c8f178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 8166\n",
      "필터링 후의 질문 샘플 개수: 9105\n",
      "필터링 후의 답변 샘플 개수: 9105\n"
     ]
    }
   ],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95cca95",
   "metadata": {},
   "source": [
    "### POSITIONAL ENCODING, ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52a54b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        # 각도 배열 생성\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "\n",
    "        # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        # sin과 cosine이 교차되도록 재배열\n",
    "        pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "        pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "        pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70f6f7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # 가중치를 정규화\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # 패딩에 마스크 추가\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # softmax적용\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ecf93dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "      # head 나누는 함수\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "            'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # Q, K, V에 각각 Dense를 적용합니다\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # 스케일드 닷 프로덕트 어텐션 함수\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))\n",
    "\n",
    "        # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce9ff80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩 마스크 구현\n",
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fcf702d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 룩어헤드 마스크 구현\n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea61b947",
   "metadata": {},
   "source": [
    "---\n",
    "## 4 모델 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "847886a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "    # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    " \n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention\")({\n",
    "            'query': inputs,\n",
    "            'key': inputs,\n",
    "            'value': inputs,\n",
    "            'mask': padding_mask\n",
    "        })\n",
    "\n",
    "    # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74752421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "    # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    # num_layers만큼 쌓아올린 인코더의 층.\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"encoder_layer_{}\".format(i),\n",
    "        )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4aab7d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention1 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "            'query': inputs,\n",
    "            'key': inputs,\n",
    "            'value': inputs,\n",
    "            'mask': look_ahead_mask\n",
    "        })\n",
    "\n",
    "    # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "    attention2 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "            'query': attention1,\n",
    "            'key': enc_outputs,\n",
    "            'value': enc_outputs,\n",
    "            'mask': padding_mask\n",
    "        })\n",
    "\n",
    "    # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "    # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d9e78e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "    # 패딩 마스크\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  \n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "  \n",
    "    # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "          inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "          outputs=outputs,\n",
    "          name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b4d4f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교사 강요 사용하기\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de388618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의 \n",
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    # 인코더에서 패딩을 위한 마스크\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
    "  # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask,\n",
    "        output_shape=(1, None, None),\n",
    "        name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "  # 디코더에서 패딩을 위한 마스크\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "       name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # 인코더\n",
    "    enc_outputs = encoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "  # 디코더\n",
    "    dec_outputs = decoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e21a5add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    3144704     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    3672064     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8166)   2098662     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,915,430\n",
      "Trainable params: 8,915,430\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6d7a7c",
   "metadata": {},
   "source": [
    "---\n",
    "## 5 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af1e3038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "783e3a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀된 학습률 \n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "01ab1e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ecb32adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일 \n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd82fdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "143/143 [==============================] - 10s 26ms/step - loss: 5.6004 - accuracy: 0.0820\n",
      "Epoch 2/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 4.7187 - accuracy: 0.1743\n",
      "Epoch 3/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 3.8986 - accuracy: 0.2158\n",
      "Epoch 4/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 3.4971 - accuracy: 0.2219\n",
      "Epoch 5/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 3.2866 - accuracy: 0.2311\n",
      "Epoch 6/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 3.0955 - accuracy: 0.2429\n",
      "Epoch 7/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 2.9002 - accuracy: 0.2575\n",
      "Epoch 8/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 2.6850 - accuracy: 0.2791\n",
      "Epoch 9/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 2.4523 - accuracy: 0.3055\n",
      "Epoch 10/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 2.1907 - accuracy: 0.3373\n",
      "Epoch 11/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 1.9144 - accuracy: 0.3710\n",
      "Epoch 12/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 1.6311 - accuracy: 0.4069\n",
      "Epoch 13/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 1.3532 - accuracy: 0.4452\n",
      "Epoch 14/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 1.0873 - accuracy: 0.4830\n",
      "Epoch 15/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.8429 - accuracy: 0.5223\n",
      "Epoch 16/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.6330 - accuracy: 0.5560\n",
      "Epoch 17/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.4595 - accuracy: 0.5861\n",
      "Epoch 18/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.3253 - accuracy: 0.6102\n",
      "Epoch 19/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.2290 - accuracy: 0.6271\n",
      "Epoch 20/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.1697 - accuracy: 0.6357\n",
      "Epoch 21/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.1343 - accuracy: 0.6404\n",
      "Epoch 22/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.1116 - accuracy: 0.6444\n",
      "Epoch 23/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.1012 - accuracy: 0.6451\n",
      "Epoch 24/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.0945 - accuracy: 0.6453\n",
      "Epoch 25/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0897 - accuracy: 0.6457\n",
      "Epoch 26/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.0861 - accuracy: 0.6459\n",
      "Epoch 27/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0826 - accuracy: 0.6466\n",
      "Epoch 28/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.0853 - accuracy: 0.6463\n",
      "Epoch 29/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.0807 - accuracy: 0.6468\n",
      "Epoch 30/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.0743 - accuracy: 0.6492\n",
      "Epoch 31/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0663 - accuracy: 0.6503\n",
      "Epoch 32/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.0611 - accuracy: 0.6516\n",
      "Epoch 33/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0565 - accuracy: 0.6526\n",
      "Epoch 34/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.0500 - accuracy: 0.6543\n",
      "Epoch 35/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.0460 - accuracy: 0.6554\n",
      "Epoch 36/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0447 - accuracy: 0.6556\n",
      "Epoch 37/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0404 - accuracy: 0.6570\n",
      "Epoch 38/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0384 - accuracy: 0.6575\n",
      "Epoch 39/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0367 - accuracy: 0.6576\n",
      "Epoch 40/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0325 - accuracy: 0.6588\n",
      "Epoch 41/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0317 - accuracy: 0.6588\n",
      "Epoch 42/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.0319 - accuracy: 0.6589\n",
      "Epoch 43/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.0300 - accuracy: 0.6594\n",
      "Epoch 44/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0272 - accuracy: 0.6600\n",
      "Epoch 45/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0264 - accuracy: 0.6600\n",
      "Epoch 46/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0262 - accuracy: 0.6605\n",
      "Epoch 47/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0229 - accuracy: 0.6608\n",
      "Epoch 48/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0241 - accuracy: 0.6606\n",
      "Epoch 49/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0220 - accuracy: 0.6612\n",
      "Epoch 50/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0227 - accuracy: 0.6612\n",
      "Epoch 51/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.0216 - accuracy: 0.6613\n",
      "Epoch 52/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0212 - accuracy: 0.6613\n",
      "Epoch 53/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.0185 - accuracy: 0.6618\n",
      "Epoch 54/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.0185 - accuracy: 0.6618\n",
      "Epoch 55/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0175 - accuracy: 0.6623\n",
      "Epoch 56/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0170 - accuracy: 0.6624\n",
      "Epoch 57/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0166 - accuracy: 0.6625\n",
      "Epoch 58/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.0160 - accuracy: 0.6624\n",
      "Epoch 59/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.0154 - accuracy: 0.6628\n",
      "Epoch 60/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0151 - accuracy: 0.6628\n",
      "Epoch 61/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.0140 - accuracy: 0.6629\n",
      "Epoch 62/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.0150 - accuracy: 0.6629\n",
      "Epoch 63/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0148 - accuracy: 0.6627\n",
      "Epoch 64/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.0138 - accuracy: 0.6628\n",
      "Epoch 65/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.0144 - accuracy: 0.6629\n",
      "Epoch 66/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.0128 - accuracy: 0.6631\n",
      "Epoch 67/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.0122 - accuracy: 0.6634\n",
      "Epoch 68/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0118 - accuracy: 0.6634\n",
      "Epoch 69/100\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 0.0119 - accuracy: 0.6633\n",
      "Epoch 70/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0129 - accuracy: 0.6632\n",
      "Epoch 71/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0124 - accuracy: 0.6633\n",
      "Epoch 72/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0120 - accuracy: 0.6632\n",
      "Epoch 73/100\n",
      "143/143 [==============================] - 4s 26ms/step - loss: 0.0118 - accuracy: 0.6633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ae51fe9a610>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# EarlyStopping 설정\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"loss\",      # loss 기준으로 모니터링\n",
    "    patience=5,          # 5 epoch 동안 개선이 없으면 멈춤\n",
    "    restore_best_weights=True  # 가장 좋은 가중치 복원\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "EPOCHS = 100\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d55cb610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "def decoder_inference(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "    sentence = tf.expand_dims(\n",
    "        START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 인퍼런스 단계\n",
    "    for i in range(MAX_LENGTH):\n",
    "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "        predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # 현재 예측한 단어의 정수\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output_sequence, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64e347b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence):\n",
    "  # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "    prediction = decoder_inference(sentence)\n",
    "\n",
    "  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "        [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    print('입력 : {}'.format(sentence))\n",
    "    print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4c50207a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 오늘 하루가 참 기네\n",
      "출력 : 다시 아침이 올 거예요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'다시 아침이 올 거예요 .'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('오늘 하루가 참 기네')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a88bbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 공부가 너무 재밌어\n",
      "출력 : 지금처럼 잘될 거예요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'지금처럼 잘될 거예요 .'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('공부가 너무 재밌어')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88d7932e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 주말에 여행 가려고\n",
      "출력 : 즐거운 주말 보내실 것 같네요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'즐거운 주말 보내실 것 같네요 .'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('주말에 여행 가려고')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecdd7ed",
   "metadata": {},
   "source": [
    "## 회고\n",
    "- 실제로 챗봇 만들고 하니까 재밌있습니다.\n",
    "- 교사 강요 사용하기 이 부분이 제대로 이해를 하지 못했는데, 프로젝트를 통해 이해를 할 수 있었다.\n",
    "- 데이터 셋이 워낙 깔끔하게 되어있어서 결과가 좋게 나온 것 같습니다.\n",
    "- QVK값에 대해서 코드로 실제로 수행하고, flow를 따라가도 보니까 다음 프로젝트에도 적용 가능 할 것 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1554cdd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
